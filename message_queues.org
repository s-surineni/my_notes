* generic
** pub/sub systems <?>
** streaming usecase
** high throughput use cases
** handling background jobs
** do we send acknowledgement <?>
* kafka
** built for streaming scenarios
** durable, fast and scalable
*** TODO can we get some real values on this <do>
** durable message store
** dumb broker smart consumer
** needs apache zookeeper to run
** uses pull model
** Allows users to leverage batching
** much higher performance than rabbitmq

** use cases
*** metrics
*** activity tracking
*** log aggregation
*** stream processing
*** commit logs
*** event sourcing
*** if we need stream history
* rabbitmq
** communication patterns
*** uses variations of request/reply, point to point, and pub-sub communication patterns.
** Smart broker / dumb consumer model
** can be synchronous too <?>
** build for high-ingress replay and streams
** allows persistance and re-process of streamed data
** uses push model
*** a prefetch limit is specified on consumer
** use cases
*** With our current code once RabbitMQ delivers message to the consumer it immediately marks it for deletion. 
*** if you kill a worker we will lose the message it was just processing.
*** In order to make sure a message is never lost, RabbitMQ supports message acknowledgments. 
*** If a consumer dies (its channel is closed, connection is closed, or TCP connection is lost) without sending an ack
*** RabbitMQ will understand that a message wasn't processed fully and will re-queue it.
*** There aren't any message timeouts; RabbitMQ will redeliver the message when the consumer dies.
*** It's fine even if processing a message takes a very, very long time.
*** Acknowledgement must be sent on the same channel that received the delivery.
**** message durability
***** our tasks will still be lost if RabbitMQ server stops.
***** When RabbitMQ quits or crashes it will forget the queues and messages unless you tell it not to. 
***** we need to mark both the queue and messages as durable.
***** Fair dispatch
