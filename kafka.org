* use cases
** capabilities
*** publish and subscribe
*** store streams of records
*** Process streams of records as they occur.
** replacement for message broker
*** better throughput
*** built in partitioning
*** replication
*** fault-tolerance
** web activity tracking
** metrics
** log aggregation
** stream processing
* only metadata retained on a per-consumer basis is the offset or position of that consumer in the log
** This offset is controlled by the consumer
** Kafka consumers are very cheap—they can come and go without much impact on the cluster or on other consumers.
* The partitions in the log serve several purposes
** allow the log to scale beyond a size that will fit on a single server
** Second they act as the unit of parallelism
** partition is replicated across a configurable number of servers for fault tolerance.
** Each partition has one server which acts as the "leader" and zero or more servers which act as "followers".
** I think partitions have different data
** but if there are more than one kafka server the partitions get copied to those servers
** if you have more consumers than partitions some of the consumers will be “starved” and not receive any messages
** consumers don’t share partitions (unless they are in different consumer groups).
** Partitions are spread across the nodes in a Kafka cluster.
** Message ordering in Kafka is per partition only.
** If you are using an (optional) message key (required for event ordering within partitions, otherwise events are round-robin load balanced across the partitions – and therefore not ordered), then you need to ensure you have many more distinct keys (> 20 is a good start) than partitions otherwise partitions may get unbalanced, and in some cases may not even have any messages (due to hash collisions).
* producers
** The producer is responsible for choosing which record to assign to which partition within the topic.
* consumers
** Consumers label themselves with a consumer group name
** each record published to a topic is delivered to one consumer instance within each subscribing consumer group.
** If all the consumer instances have the same consumer group, then the records will effectively be load balanced over the consumer instances.
** If all the consumer instances have different consumer groups, then each record will be broadcast to all the consumer processes
** The way consumption is implemented in Kafka is by dividing up the partitions in the log over the consumer instances so that each instance is the exclusive consumer of a "fair share" of partitions at any point in time
** Kafka only provides a total order over records within a partition, not between different partitions in a topic
** each consumer within the group reads from a unique partition and the group as a whole consumes all messages from the entire topic.
* advantages
** scalability
** reliability
** performance
** fault tolerance
*** because message is not gone after consumer consumes it
* most messaging system are implemented with single host
** chance of broker getting full and unable to take any more messages
* message broker is bottleneck?
* messaging systems are considered middleware
* things messaging system manage
** distributed co-ordination logic
** multi face commits
** error handling
** data consistency handling
* multi write pattern
* kafka goals
** high throughput
** horizontally scalable
** reliable and durable
** loosely coupled producers and consumers
* kafka architecture
** messaging system
** pub-sub
** producers consumers topic
*** broker houses all these
* larger messages put strain on brokers
* broker is a process that runs on a machine
* achieving high throughput is largely a function of how well a system can distribute its load and efficiently process it on multiple nodes in parallel
* we can add as many brokers as possible
** adding more brokers wont affect existing producer and consumer
* kafka cluster is grouping of multiple brokers
* proper coordination is required to distribute work properly
* How brokers are managed
** There will be one controller which is also a broker
** Responsibilities of a controller
*** maintaining list of brokers
*** maintains list of work items to be distributed among workers
*** maintaining status of workers and progress on tasks assigned to them
** steps in assigning new work to brokers
*** check which brokers are available
*** the risk policy it should use
**** redundancy level
*** distributed system may offer redundancy options
**** it needs to make sure the work assigned or work already done is not lost
**** because of this same work is usually given to multiple workers
*** How is the replication achieved?
**** Controller will select leaders
**** leader will ask if peers will take part in replication
**** if leader gets quorum, then those peers will participate in replication
* in distributed systems every system has to communicate with each other
** formal name is consensus or gossip protocol
** example when new broker want to join the cluster
** configuration management
** leader election
** health status
* zookeeper maintains metadata about cluster of distributed nodes
** configuration information
** health status
** group membership
* chances of failure
** broker failure
** network issue
** disk failure
* each broker is leader for one or more partitions
* when zookeeper notices that one broker is down it selects another broker to take its place
** metadata used for work distribution gets updated
** replication guarantees reliable work distribution
** replication factor should be  2 to 3
** with replication factor of 3 leaders establish quorum with peers and copy the partition messages to peers
** if quorum cannot be established or number of in sync replicas are below replication factor intervantions are required
* 2 different ways of specifying timestamp
* importance of key
** information in message that can be used to take processing decisions later
** can determine what partitions a message will go to
